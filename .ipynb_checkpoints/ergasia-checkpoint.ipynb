{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeJxAKSvNfNy"
   },
   "source": [
    "# IMDB sentiment analysis with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhIK8UDAgpMu"
   },
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8RMZkur0Z7H4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=4000)\n",
    "\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quKpVbcFEhZT",
    "outputId": "30438610-f123-4e4a-e09c-a21115540867"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "AOEmWY7yfBFu",
    "outputId": "7a134933-2827-4e02-dd3f-651ce21c77d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[bos] this film was just brilliant casting location scenery story direction [oov] really suited the part they played and you could just imagine being there robert [oov] is an amazing actor and now the same being director [oov] father came from the same [oov] island as myself so i loved the fact there was a real connection with this film the witty [oov] throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for [oov] and would recommend it to everyone to watch and the fly [oov] was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also [oov] to the two little [oov] that played the [oov] of norman and paul they were just brilliant children are often left out of the [oov] list i think because the stars that play them all grown up are such a big [oov] for the whole film but these children are amazing and should be [oov] for what they have done don't you think the whole story was so lovely because it was true and was [oov] life after all that was [oov] with us all\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYTO-LIY0j-h"
   },
   "source": [
    "## Alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRkm4epU0moB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open ''aclImdb_v1.tar.gz''\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xvf  'aclImdb_v1.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoCXNaVXgmXN"
   },
   "source": [
    "## Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP5Xpgpcgl0_",
    "outputId": "e9cee4ea-3b1e-4c87-f370-6e7872726c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = list()\n",
    "train_words = list()\n",
    "sorted_words = list()\n",
    "for text in x_train:\n",
    "  tokens = text.split()\n",
    "  train_words.extend(tokens)\n",
    "\n",
    "Counter = Counter(train_words)\n",
    "Counter_copy = Counter\n",
    "temp = Counter.most_common(3998)\n",
    "for key in temp:\n",
    "    sorted_words.append(key[0])\n",
    "\n",
    "#n=99, m = 1000, k = 2898\n",
    "k=list()\n",
    "n=list()\n",
    "m = Counter.most_common(1100)\n",
    "j=0\n",
    "for key in m:\n",
    "  j+=1\n",
    "  if(j>=101):\n",
    "    vocabulary.append(key[0])\n",
    " \n",
    "\n",
    "for i in range(1,100):\n",
    "  n.append(sorted_words[i])\n",
    "j=0\n",
    "for key in sorted_words:\n",
    "  j+=1\n",
    "  if(j<=1100):\n",
    "    sorted_words.remove(key)\n",
    "k = sorted_words.copy()\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDGjmz4UxlRi"
   },
   "source": [
    "## Create binary vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KESrOUVAhmRD",
    "outputId": "077cba85-f8c7-4070-a896-fe7d84e1c4c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:10<00:00, 355.53it/s]\n",
      "100%|██████████| 25000/25000 [01:07<00:00, 371.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x_train_binary = list()\n",
    "x_test_binary = list()\n",
    "\n",
    "for text in tqdm(x_train):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_train_binary.append(binary_vector)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary)\n",
    "\n",
    "for text in tqdm(x_test):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_test_binary.append(binary_vector)\n",
    "\n",
    "x_test_binary = np.array(x_test_binary)\n",
    "#print(x_test_binary[0])\n",
    "y_train_list = y_train.tolist()\n",
    "print(x_train_binary[0])\n",
    "\n",
    "vocabulary_indexes = list()\n",
    "for i in range(len(vocabulary)):\n",
    "  vocabulary_indexes.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh0UFy2FyCW4"
   },
   "source": [
    "## Naive Bayes classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT5E13ejyD2f",
    "outputId": "64341573-8b80-43a2-88b4-7f50c404c8a3"
   },
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "\n",
    "    def __init__(self):\n",
    "        #Λίστες με τις πιθανότητες να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review αντίστοιχα\n",
    "        self.x1_while_c_is_negative = list()\n",
    "        self.x1_while_c_is_positive = list()\n",
    "        #καθολικές μεταβλητές με την πιθανότητα να έχουμε αρνητικό review και θετικό review αντιστοιχα\n",
    "        self.p_c0 = float(0)\n",
    "        self.p_c1 = float(0)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        reviews = len(Y) \n",
    "\n",
    "        #Υπολογισμός της γενικής πιθανότητας να έχουμε θετικό ή αρνητικό review:\n",
    "        neg_reviews = 0 \n",
    "        pos_reviews = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == 0:\n",
    "                neg_reviews += 1\n",
    "            else:\n",
    "                pos_reviews += 1\n",
    "        p_positive_reviews = pos_reviews/reviews\n",
    "        p_negative_reviews = neg_reviews/reviews\n",
    "        #Αποθήκευση του αποτελέσματος τις καθολικές μεταβλητές\n",
    "        self.pc0 = p_negative_reviews\n",
    "        self.pc1 = p_positive_reviews\n",
    "\n",
    "        #Υπολογισμός πιθανοτήτων να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review αντίστοιχα:\n",
    "        \n",
    "\n",
    "        #Αρχικοποίηση λιστών με μετρητές\n",
    "        sum_pos = list()\n",
    "        sum_neg = list()\n",
    "        for i in range(0,1000):\n",
    "            sum_pos.append(0) \n",
    "            sum_neg.append(0)\n",
    "\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == 0:\n",
    "                j=-1\n",
    "                for word in X[i]:\n",
    "                    j+=1\n",
    "                    if word == 1:\n",
    "                        sum_neg[j] +=1\n",
    "            else:\n",
    "                j=-1\n",
    "                for word in X[i]:\n",
    "                    j+=1\n",
    "                    if word == 1:\n",
    "                        sum_pos[j] +=1\n",
    "\n",
    "        #Λίστες πιθανοτήτων να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review \n",
    "        p_ex_pos = list()\n",
    "        p_ex_neg = list()\n",
    "        for i in range(0,1000):\n",
    "            p_ex_pos.append(0) \n",
    "            p_ex_neg.append(0)\n",
    "        #for i in range(len(p_ex_neg_train)):\n",
    "        for i in range(0,1000):\n",
    "            if(sum_neg[i]!=0):\n",
    "                p_ex_neg[i] = sum_neg[i]/neg_reviews #P(Xi = 1 | C = 0)\n",
    "            else:\n",
    "                p_ex_neg[i] = -1 # -1 για τις λέξεις που δεν εμφανίζονται καν\n",
    "        for i in range(0,1000):\n",
    "            if(sum_pos[i]!=0):\n",
    "                p_ex_pos[i] = sum_pos[i]/neg_reviews #P(Xi = 1 | C = 1)\n",
    "            else:\n",
    "                p_ex_pos[i] = -1 # -1 για τις λέξεις που δεν εμφανίζονται καν\n",
    "\n",
    "        self.x1_while_c_is_positive = p_ex_pos.copy()\n",
    "        self.x1_while_c_is_negative = p_ex_neg.copy()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        predictions = list()\n",
    "        for i in range(len(X)):\n",
    "\n",
    "            #Το pc0 Θα γίνει pc0 = P(C=0)*P(X1=x1 | C=0)*.......*P(Xn = xn| C=0)\n",
    "            pc0 = (self.pc0)   \n",
    "            for xi in range(len(X[i])):\n",
    "                if(x_train_binary[i][xi] == 0):\n",
    "                    pc0 *= (float(1)-self.x1_while_c_is_negative[xi])  \n",
    "                else:\n",
    "                    pc0 *= (self.x1_while_c_is_negative[xi])\n",
    "            pc1 = (self.pc1)\n",
    "            for xi in range(len(X[i])):\n",
    "                if(x_train_binary[i][xi]==1):\n",
    "                    pc1 *= self.x1_while_c_is_positive[xi]\n",
    "                else:\n",
    "                    pc1 *= float(1) - self.x1_while_c_is_positive[xi]\n",
    "            if (pc0<pc1):\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "        #c = 0\n",
    "        # pc0 = np.log2(self.pc0)   \n",
    "        # for xi in range(len(X[i])):\n",
    "        #     if(x_train_binary[i][xi] == 0):\n",
    "        #         pc0 += np.log2(float(1)-self.x1_while_c_is_negative[xi])  \n",
    "        #     else:\n",
    "        #         pc0 += np.log2(self.x1_while_c_is_negative[xi])\n",
    "\n",
    "        # #c=1\n",
    "        # pc1 = np.log2(self.pc1)\n",
    "        # for xi in range(len(X[i])):\n",
    "        #     if(x_train_binary[i][xi]==1):\n",
    "        #         pc1 += np.log2(self.x1_while_c_is_positive[xi])\n",
    "        #     else:\n",
    "        #         pc1 += np.log2(float(1) - self.x1_while_c_is_positive[xi])\n",
    "        # if (pc0<pc1):\n",
    "        #     predictions.append(1)\n",
    "        # else:\n",
    "        #     predictions.append(0)\n",
    "        #__________________________________________________________________________\n",
    "        # y_pred = list()\n",
    "        # for i in range (len(X)):\n",
    "        #     #pithanothta thetikou review:\n",
    "        #     j=-1\n",
    "        #     p_its_positive = 1\n",
    "        #     for word in X[i]:\n",
    "        #         j+=1\n",
    "        #         if(word == 1):\n",
    "        #             p_word = self.x1_while_c_is_positive[j]\n",
    "        #             p_its_positive= p_its_positive*p_word\n",
    "        #         elif(word == 0):\n",
    "        #             p_word = (1 - self.x1_while_c_is_positive[j])\n",
    "        #             p_its_positive= p_its_positive*p_word\n",
    "        #     p_its_positive = p_its_positive*self.p_c1\n",
    "            \n",
    "        #     #pithanothta arnhtikou review\n",
    "        #     j=-1\n",
    "        #     p_its_negative = 1\n",
    "        #     for word in X[i]:\n",
    "        #         j+=1\n",
    "        #         if(word == 1):\n",
    "        #             p_word = self.x1_while_c_is_negative[j]\n",
    "        #             p_its_negative= p_its_negative*p_word\n",
    "        #         elif(word == 0):\n",
    "        #             p_word = (1 - self.x1_while_c_is_negative[j])\n",
    "        #             p_its_negative= p_its_negative*p_word\n",
    "        #     p_its_negative = p_its_negative*self.p_c0\n",
    "\n",
    "        #     for i in range(len(X)):\n",
    "        #         if p_its_negative>p_its_positive:\n",
    "        #             y_pred.append(0)\n",
    "        #         else:\n",
    "        #             y_pred.append(1)\n",
    "        \n",
    "        # return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.44\n",
      "49.536\n",
      "12384\n"
     ]
    }
   ],
   "source": [
    "tool = Naive_Bayes()\n",
    "y_train_list = y_train.tolist()\n",
    "y_test_list = y_test.tolist()\n",
    "tool.fit(x_train_binary, y_train_list)\n",
    "y_pred = tool.predict(x_train_binary)\n",
    "sum=0\n",
    "for i in range(len(y_train_list)):\n",
    "    if(y_train_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_train_list))*100\n",
    "print(correct_percentage_test) \n",
    "\n",
    "# sum = 0\n",
    "# p1 = tool.x1_while_c_is_negative.copy()\n",
    "# p2 = tool.x1_while_c_is_positive.copy()\n",
    "tool2 = Naive_Bayes()\n",
    "# tool2.fit(x_test_binary, y_test_list)\n",
    "y_pred = tool.predict(x_test_binary)\n",
    "sum=0\n",
    "for i in range(len(y_train_list)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_train_list))*100\n",
    "print(correct_percentage_test) \n",
    "\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqYg0fgfyaQY",
    "outputId": "49019cae-25df-4135-a434-c0d9a306b04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83     12500\n",
      "           1       0.82      0.86      0.84     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.84      0.83      0.83     25000\n",
      "weighted avg       0.84      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_train, nb.predict(x_train_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmh2Gxc-ys1h"
   },
   "source": [
    "## ID3 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeOd5C9uyyqI",
    "outputId": "65c0cd08-18e3-4630-b19c-7de464308bab"
   },
   "outputs": [],
   "source": [
    "def IG(Y, Xi):\n",
    "    \n",
    "    pc0=list(Y).count(0)/len(Y) #P(C=0) \n",
    "    pc1=list(Y).count(1)/len(Y) #P(C=1)\n",
    "\n",
    "    #H(C) = -P(C=0)*log2(P(C=0)) -P(C=1)*log2(P(C=1))\n",
    "    Hc= -(pc0) * (math.log(pc0, 2)) -(pc1) * (math.log(pc1, 2))\n",
    "\n",
    "    #P(Xi = 1)*H(C|Xi=1) + P(Xi = 0)*H(C|Xi=0)\n",
    "    sum_calculations = 0 \n",
    "    for digit in range(0,2):\n",
    "\n",
    "        #P(Xi = digit)\n",
    "        p = list(Xi).count(digit)/len(Xi)\n",
    "\n",
    "        #H(C|Xi=digit)\n",
    "        Y_Xi = list()\n",
    "        for i in range(len(Xi)):\n",
    "            if Xi[i]==digit:\n",
    "                Y_Xi.append(Y[i])\n",
    "        if(len(Y_Xi)!=0):\n",
    "            for digit2 in range(0,2):\n",
    "                #H(C=digit2|Xi=digit)\n",
    "                pc = Y_Xi.count(digit2)/len(Y_Xi)\n",
    "                if(pc != 0):\n",
    "                    sum_calculations += p * pc * np.log2(pc)\n",
    "\n",
    "    #IG(Y,Xi)= H(C) - P(Xi = 1)*H(C|Xi=1) + P(Xi = 0)*H(C|Xi=0)\n",
    "    ig = Hc - sum_calculations\n",
    "    return ig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Tree():\n",
    "    def __init__(self):\n",
    "        self.word = \"no word yet\" #Η λέξη με την οποία θα έγινε το classification ενός υπόδεντρου\n",
    "        self.tag = None #1 αν ο κόμβος έχει reviews με τη λέξη με την οποία έγινε το classification, 0 αν δεν την έχουν\n",
    "        self.children = list() #τα παιδία ενός κόμβου\n",
    "        self.classification = int #Η τελική classification. Παίρνει τιμή μόνο αν έχει γίνει \n",
    "    \n",
    "    def new_child(self, node):\n",
    "        self.children.append(node)\n",
    "\n",
    "class ID3():\n",
    "    def __init__(self, max_depth = 10):\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = 0\n",
    "\n",
    "    def most_IG(self, X, Y, vocabulary):\n",
    "\n",
    "        max_gain = -1\n",
    "        max_word= -1\n",
    "\n",
    "\n",
    "        for w in vocabulary:\n",
    "            x_word = list() #Λίστα με όλες τις τιμές που θα πάρει μια λέξη στον Χ\n",
    "            for ex in range(len(X)):\n",
    "                x_word.append(X[ex][w])\n",
    "            word_ig = IG(Y, x_word) #Στέλνουμε το Υ και την λίστα στον ΙG, για να βρει το informtion gain της λέξης ανάλογικά με το Υ\n",
    "\n",
    "            if(word_ig>max_gain):\n",
    "                max_gain = word_ig\n",
    "                max_word = w\n",
    "\n",
    "        return max_word #Η λέξη με το μέγιστο Information Gain\n",
    "\n",
    "    def fit(self, X, Y, vocabulary, default):\n",
    "       \n",
    "        if(len(Y) == 0):\n",
    "            #αν φτάσαμε εδώ τελείωσαν τα Υ γιατί τελείωσε η κατάταξη κάθε review στο δέντρο.\n",
    "            #Παίρνει για τιμή του classification εκείνη που επικρατούσε στο παραπάνω επίπεδο του δέντρο\n",
    "            node = Tree()\n",
    "            node.classification = default \n",
    "            return node \n",
    "\n",
    "        if(len(set(Y)) == 1):\n",
    "            #Η μέθοδος set επιστρέφει ένα set με όλες τις διαφορετικές τιμές που περιλαμβάνει το όρισμα της, εδώ το Υ\n",
    "            #Άρα φτάσαμε εδώ αν το Υ έχει μόνο μια τιμη, η οποία θα χρησιμοποιηθεί στο classification και σταματάει η διαδικασία. \n",
    "            node = Tree()\n",
    "            node.classification = Y[0]\n",
    "            return node\n",
    "\n",
    "        if(len(vocabulary) == 0):\n",
    "            #Αν φτάσαμε εδώ χρησιμοποιήσαμε όλες τις λέξεις οπότε δεν γίνονται παραπάνω κατατάξεις.\n",
    "            #Η διαδικασία σταματάει και γίνεται classified με την τιμή που επικρατεί στα Y\n",
    "            node = Tree()\n",
    "            if(Y.count(0)>Y.count(1)):\n",
    "                max_count = 0\n",
    "            else:\n",
    "                max_count = 1\n",
    "            node.classification = max_count\n",
    "            return node\n",
    "\n",
    "        if (self.depth == self.max_depth):\n",
    "            #Αν είμαστε εδώ φτάσαμε το max depth του δέντρου\n",
    "            #Η διαδικασία σταματάει και γίνεται classified με την τιμή που επικρατεί στα Y. \n",
    "            #Αν έχουμε ισοπαλία αρνητικών θετικών reviews παίρνουμε το default, δηλαδή αυτή που επικρατούσε στο παραπάνω επίπεδο\n",
    "            yes = True\n",
    "            node = Tree()\n",
    "            if((Y.count(0))>Y.count(1)):\n",
    "                node.classification = 0\n",
    "            elif((Y.count(0))<Y.count(1)):\n",
    "                node.classification = 1\n",
    "            else:\n",
    "                node.classification =default\n",
    "            return node\n",
    "\n",
    "        #Σταματάει η διαδικασία αν υπερτερεί στα εναπομείναντα reviews είτε το 0 είτε το 1\n",
    "        if (float(Y.count(1))/float(len(Y))>= 0.95):\n",
    "            node = Tree()\n",
    "            node.classification = 1                   \n",
    "            return node\n",
    "        \n",
    "        if(float(Y.count(0))/float(len(Y))>= 0.95):\n",
    "            node = Tree()\n",
    "            node.classification = 0\n",
    "            return node\n",
    "\n",
    "        #Αποθήκευση του clssification που επικρατεί μέχρι στιγμης ώστε να περασθεί ως default στα παρακάτω επίπεδα\n",
    "        if(Y.count(1)>Y.count(0)):\n",
    "            max_count = 1\n",
    "        else:\n",
    "            max_count = 0\n",
    "\n",
    "\n",
    "        best_word = self.most_IG(X, Y, vocabulary) #Εύρεση της λέξης με το μέγιστο Information Gain \n",
    "        tree = Tree() #Αρχικοποίηση υπόδεντρου\n",
    "\n",
    "        #το νεο λεξιλόγιο, χωρίς την λέξη που θα χρσιμοποιηθέι για τον διαχωρισμό σε φύλλα τωρα ωστέ να μην ξαναχρησιμοποιηθεί μετά\n",
    "        new_vocabulary = vocabulary.copy() \n",
    "        new_vocabulary.remove(best_word)\n",
    "        self.depth += 1 #ενημέρωση του depth\n",
    "\n",
    "        for zero_or_one in range(2):\n",
    "            # if(zero_or_one==0):\n",
    "            #     print(len(new_vocabulary))\n",
    "            #Oι νέες λίστες reviews, δημιουργούνται 2 για κάθε κατηγορία(Υ και Χ) λόγω της for, μια με τη best_word και μία χωρις \n",
    "            x_new = list()\n",
    "            y_new = list()\n",
    "            for i in range(len(X)):\n",
    "                if X[i][best_word] == zero_or_one:\n",
    "                    x_new.append(X[i])\n",
    "                    y_new.append(Y[i])\n",
    "            subtree = self.fit(x_new, y_new, new_vocabulary, max_count)\n",
    "            subtree.tag = zero_or_one \n",
    "            subtree.word = best_word\n",
    "            tree.new_child(subtree)            \n",
    "                \n",
    "        return tree\n",
    "\n",
    "    # def predict_sample(self, x_sample, tree):\n",
    "    #     decided = False\n",
    "    #     sub_tree = tree\n",
    "    #     while not decided:\n",
    "    #         feature = sub_tree.children[0].feature\n",
    "    #         for sub in sub_tree.children:\n",
    "    #             if sub.tag == x_sample[feature]:\n",
    "    #                 sub_tree = sub\n",
    "    #         if (sub_tree.decision == 1 or sub_tree.decision == 0):\n",
    "    #             decided = True\n",
    "    #     return sub_tree.decision\n",
    "\n",
    "    def singular_prediction(self, X, tree):\n",
    "        sub_tree = tree #Αρχικοποίηση Υπόδεντρου\n",
    "        flag = False\n",
    "        while not flag:\n",
    "            word_feature = sub_tree.children[0].word #Παίρνουμε τη λέξη με την οποία έγινε ο διαχωρισμός\n",
    "            for sub in sub_tree.children:\n",
    "                if (sub.tag == X[word_feature]): \n",
    "                    #Αν υπάρχει η λέξη-κριτήριο με την οποία έγινε ο διαχωρισμος σε αυτο το επίπεδο στο sample που κοιταμε \n",
    "                    # πάμε στο υπόδεντρο οπόυ το tag είναι 1(δηλ. έχει reviews που την περιλαμβανουν), αλλιώς σε αυτο που ειναι 0\n",
    "                    sub_tree = sub\n",
    "            if(sub_tree.classification == 1 or sub_tree.classification == 0):\n",
    "                #Σταματάμε αν φτάσουμε σε κάποιο φυλλο. Τα φύλλα έχουν τιμή 0 ή 1 και αυτο καταλήγει να ειναι το classification\n",
    "                # του sample. Οι άλλοι ενδιάμεσοι κόμβοι έχουν None στο classification\n",
    "                flag = True\n",
    "        return sub_tree.classification\n",
    "\n",
    "    def predict(self, tree, X):\n",
    "        y_pred = list()\n",
    "        for i in range(len(X)):\n",
    "            y_pred.append(self.singular_prediction(X[i], tree)) #πρόβλεψη για κάθε review ξεχωριστά\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[0;32m      4\u001b[0m dt \u001b[39m=\u001b[39m DecisionTreeClassifier(criterion\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\__init__.py:82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     85\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_set_output\u001b[39;00m \u001b[39mimport\u001b[39;00m _SetOutputMixin\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_tags\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     _DEFAULT_TAGS,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\__init__.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdiscovery\u001b[39;00m \u001b[39mimport\u001b[39;00m all_estimators\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     as_float_array,\n\u001b[0;32m     29\u001b[0m     assert_all_finite,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     _is_arraylike_not_scalar,\n\u001b[0;32m     39\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\fixes.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\stats\\distributions.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_continuous_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_levy_stable\u001b[39;00m \u001b[39mimport\u001b[39;00m levy_stable\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\stats\\_discrete_distns.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m entr, logsumexp, betaln, gammaln \u001b[39mas\u001b[39;00m gamln, zeta\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m _lazywhere, rng_integers\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpolate\u001b[39;00m \u001b[39mimport\u001b[39;00m interp1d\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m floor, ceil, log, exp, sqrt, log1p, expm1, tanh, cosh, sinh\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\interpolate\\__init__.py:175\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fitpack2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_rbf\u001b[39;00m \u001b[39mimport\u001b[39;00m Rbf\n\u001b[1;32m--> 175\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_rbfinterp\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    177\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_polyint\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    179\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_cubic\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\interpolate\\_rbfinterp.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m comb\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlapack\u001b[39;00m \u001b[39mimport\u001b[39;00m dgesv  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_rbfinterp_pythran\u001b[39;00m \u001b[39mimport\u001b[39;00m (_build_system,\n\u001b[0;32m     12\u001b[0m                                  _build_evaluation_coefficients,\n\u001b[0;32m     13\u001b[0m                                  _polynomial_matrix)\n\u001b[0;32m     16\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mRBFInterpolator\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[39m# These RBFs are implemented.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(x_train_binary, y_train)\n",
    "y_pred = dt.predict(x_test_binary)\n",
    "y_test_list = y_test.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)\n",
    "#print(classification_report(y_train, dt.predict(x_train_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "# clf = ID3()\n",
    "# tree = clf.fit(x_train_binary, y_train, list(range(len(x_train_binary[0]))), 1)\n",
    "# prediction = clf.predict(tree, x_test_binary)\n",
    "#meta th fit:\n",
    "#y_pred = model.predict(trained_tree, x_train_binary)\n",
    "# y_train_list = y_train.tolist()\n",
    "# sum=0\n",
    "# for i in range(len(y_train)):\n",
    "#     if(y_train_list[i]==y_pred[i]):\n",
    "#         sum+=1\n",
    "# correct_percentage_train = (sum/len(y_train_list))*100\n",
    "# print(correct_percentage_train)\n",
    "\n",
    "# print(len(vocabulary_indexes))\n",
    "model = ID3(400)\n",
    "y_train_list = y_train.tolist()\n",
    "y_test_list = y_test.tolist()\n",
    "trained_tree = model.fit(x_train_binary, y_train_list, vocabulary_indexes, 0)\n",
    "\n",
    "y_pred = model.predict(trained_tree, x_train_binary)\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_train_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)\n",
    "\n",
    "#trained_tree = model.fit(x_test_binary, y_test_list, vocabulary_indexes, 0)\n",
    "y_pred = model.predict(trained_tree, x_test_binary)\n",
    "y_test_list = y_test.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNfgLhBNIqLc",
    "outputId": "2e862136-b144-4aba-df7e-dd147a18c973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12500\n",
      "           1       1.00      1.00      1.00     12500\n",
      "\n",
      "    accuracy                           1.00     25000\n",
      "   macro avg       1.00      1.00      1.00     25000\n",
      "weighted avg       1.00      1.00      1.00     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, dt.predict(x_train_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wt6yoSICzPpz",
    "outputId": "a429d3db-2833-43af-9347-2c4f8a0203af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71     12500\n",
      "           1       0.71      0.71      0.71     12500\n",
      "\n",
      "    accuracy                           0.71     25000\n",
      "   macro avg       0.71      0.71      0.71     25000\n",
      "weighted avg       0.71      0.71      0.71     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, dt.predict(x_test_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhNCa4YSzS5J"
   },
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyQtFiv8zUV5",
    "outputId": "d46121a7-eecc-492e-ba8a-7cad33aa614f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Random_Forest():\n",
    "    def __init__(self, num_of_words, trees = 10):\n",
    "        self.num_of_words = num_of_words #Αριθμός των λέξεων\n",
    "        self.trees = trees #Αριθμός των δέντρων που θα φτιαχθούν\n",
    "        self.forest = list() #Λίστα Δέντων\n",
    "\n",
    "    def new_sample(self, X, Y):\n",
    "        #Αρχικοποίηση των νέων x και y\n",
    "        x_new = list()\n",
    "        y_new = list()\n",
    "\n",
    "        y_indexes = list() #Tα indexes των reviews που δεν έχουν επιλεχθεί\n",
    "        for i in range(len(Y)):\n",
    "            y_indexes.append(i)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            #Τυχαία επιλογή reviews για το υποσύνολο που θα επιστρέψει η μέθοδος, χρησιμοποιώντας τα indexes που φτιάχτηκε πάνω\n",
    "            random_choice = random.choice(y_indexes) \n",
    "            x_new.append(X[random_choice])\n",
    "            y_new.append(Y[random_choice])\n",
    "\n",
    "        return x_new, y_new\n",
    "\n",
    "    def new_vocabulary(self, X):\n",
    "        #Λίστα με τα indexes του λεξιλογιου για τυχαία επιλογή των νέων λέξεων του νέου λεξιλογίου που επιστρέφει η μέθοδος \n",
    "        words_indexes = list()\n",
    "        for x in range(len(X[0])):\n",
    "            words_indexes.append(x)\n",
    "\n",
    "        new_words = list()\n",
    "        for i in range(self.num_of_words):\n",
    "            random_word = random.choice(words_indexes) #Tυχαία επιλογή λέξης\n",
    "            words_indexes.remove(random_word) #Αφαίρεση από το παλιό λεξιλόγιο\n",
    "            new_words.append(random_word) #Εισαγωγή στο καινούριο\n",
    "\n",
    "        return new_words\n",
    "\n",
    "    def fit(self, X, Y, max_depth = 10):\n",
    "        for i in range(self.trees):\n",
    "            id3 = ID3(max_depth) #Δημιουργία id3 δέντρου\n",
    "            random_x, random_y = self.new_sample(X, Y)\n",
    "            tree = id3.fit(random_x, random_y, self.new_vocabulary(random_x), 0)\n",
    "            self.forest.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = list()\n",
    "        for i in range(len(X)):\n",
    "            zeros =0\n",
    "            ones = 0\n",
    "            for j in range(self.trees):\n",
    "                id3 = ID3()\n",
    "                prediction = id3.singular_prediction(X[i], self.forest[j])\n",
    "                if (prediction == 1):\n",
    "                    ones += 1\n",
    "                elif(prediction==0):\n",
    "                    zeros +=1\n",
    "            if ones>zeros:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit done\n",
      "fit done\n",
      "fit done\n",
      "fit done\n",
      "fit done\n",
      "fit done\n",
      "fit done\n",
      "fit done\n",
      "fit done\n",
      "fit done\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "model = Random_Forest(len(vocabulary))\n",
    "trained_forest = model.fit(x_train_binary, y_train_list)\n",
    "\n",
    "y_pred = model.predict(x_test_binary)\n",
    "y_test_list = y_test.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gT5jtyvszqyM",
    "outputId": "e4819fb5-a389-48a1-ea6e-b257eb92fcbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84     12500\n",
      "           1       0.84      0.84      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, rf.predict(x_test_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgwVr_GAzu9o"
   },
   "source": [
    "## AdaBoost classifier\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffzG0v0Wzv0w",
    "outputId": "4e53ff0b-f198-4f25-eb41-fd35115bdf4e"
   },
   "outputs": [],
   "source": [
    "# Decision stump used as weak classifier\n",
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples =np.shape(X) \n",
    "        print(n_samples)\n",
    "        X_column = X[:, self.feature_idx]\n",
    "        predictions = list()\n",
    "        for i in range(len(n_samples)):\n",
    "            predictions.append(1)\n",
    "        if self.polarity == 1:\n",
    "            for i in range(len(n_samples)):\n",
    "                if(X_column[i] < self.threshold):\n",
    "                    predictions[i] = 0\n",
    "        else:\n",
    "            for i in range(len(n_samples)):\n",
    "                if(X_column[i] > self.threshold):\n",
    "                    predictions[i] = 0\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class Adaboost():\n",
    "\n",
    "    def __init__(self, n_clf=5):\n",
    "        self.n_clf = n_clf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        n_features = len(X)\n",
    "        print(n_features)\n",
    "        # Initialize weights to 1/N\n",
    "        #w = np.full(n_samples, (1 / n_samples)) kanei to idio\n",
    "        w = list()\n",
    "        for i in range(n_samples):\n",
    "            w.append(1/n_samples)\n",
    "\n",
    "        self.clfs = list()\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "\n",
    "            min_error = float(1000000000000)\n",
    "            # greedy search to find best threshold and feature\n",
    "            #for feature_j in \n",
    "            for feature_i in range(n_features):\n",
    "                X_column = X[feature_i]\n",
    "                print(len(X_column)) \n",
    "                #print(feature_i)\n",
    "                thresholds = np.unique(X_column)\n",
    "                #print(X_column)\n",
    "                #print(X)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    # predict with polarity 1\n",
    "                    p = 1\n",
    "                    predictions = list()\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[X_column < threshold] = -1\n",
    "                    #for i in range(n_samples):\n",
    "                    #    predictions.append(1)\n",
    "                    #for i in range(n_samples):\n",
    "                    #    if(X_column < threshold):\n",
    "                    #        predictions[i] = 0\n",
    "\n",
    "                    print(predictions)\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    misclassified = list()\n",
    "                    for i in range(n_samples):\n",
    "                        if (predictions[i]!=y):\n",
    "                            misclassified.append(w[i])\n",
    "                    error = 0\n",
    "                    for i in range(len(misclassified)):\n",
    "                        error += misclassified[i]\n",
    "\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # store the best configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_idx = feature_i\n",
    "                        min_error = error\n",
    "\n",
    "            # calculate alpha\n",
    "            EPS = 1e-10\n",
    "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "            # calculate predictions and update weights\n",
    "            predictions = clf.predict(X)\n",
    "\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds, axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    sum=0\n",
    "    for i in range(len(y_true)):\n",
    "        if(y_true[i]==y_pred[i]):\n",
    "            sum+=1\n",
    "    correct_percentage_test = (sum/y_true)*100\n",
    "    return correct_percentage_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "1000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 25000 but corresponding boolean dimension is 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m clf \u001b[39m=\u001b[39m Adaboost(n_clf\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m clf\u001b[39m.\u001b[39;49mfit(x_train_binary,y_train)\n\u001b[0;32m      3\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(x_test_binary)\n\u001b[0;32m      5\u001b[0m acc \u001b[39m=\u001b[39m accuracy(y_test, y_pred)\n",
      "Cell \u001b[1;32mIn[65], line 64\u001b[0m, in \u001b[0;36mAdaboost.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     62\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[0;32m     63\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(n_samples)\n\u001b[1;32m---> 64\u001b[0m predictions[X_column \u001b[39m<\u001b[39;49m threshold] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39m#for i in range(n_samples):\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m#    predictions.append(1)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m#for i in range(n_samples):\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m#    if(X_column < threshold):\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m#        predictions[i] = 0\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 25000 but corresponding boolean dimension is 1000"
     ]
    }
   ],
   "source": [
    "clf = ID3()\n",
    "clf.fit(x_train_binary,y_train)\n",
    "y_pred = clf.predict(x_test_binary)\n",
    "\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toInvux_0Ue6",
    "outputId": "ca86519e-787b-4671-bf5f-29a5716d8125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80     12500\n",
      "           1       0.79      0.84      0.81     12500\n",
      "\n",
      "    accuracy                           0.81     25000\n",
      "   macro avg       0.81      0.81      0.81     25000\n",
      "weighted avg       0.81      0.81      0.81     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, ab.predict(x_test_binary)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sklearn-IMDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

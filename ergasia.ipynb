{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeJxAKSvNfNy"
   },
   "source": [
    "# IMDB sentiment analysis with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhIK8UDAgpMu"
   },
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "8RMZkur0Z7H4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math \n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=4000)\n",
    "\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quKpVbcFEhZT",
    "outputId": "30438610-f123-4e4a-e09c-a21115540867"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "AOEmWY7yfBFu",
    "outputId": "7a134933-2827-4e02-dd3f-651ce21c77d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[bos] this film was just brilliant casting location scenery story direction [oov] really suited the part they played and you could just imagine being there robert [oov] is an amazing actor and now the same being director [oov] father came from the same [oov] island as myself so i loved the fact there was a real connection with this film the witty [oov] throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for [oov] and would recommend it to everyone to watch and the fly [oov] was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also [oov] to the two little [oov] that played the [oov] of norman and paul they were just brilliant children are often left out of the [oov] list i think because the stars that play them all grown up are such a big [oov] for the whole film but these children are amazing and should be [oov] for what they have done don't you think the whole story was so lovely because it was true and was [oov] life after all that was [oov] with us all\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYTO-LIY0j-h"
   },
   "source": [
    "## Alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "DRkm4epU0moB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open ''aclImdb_v1.tar.gz''\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xvf  'aclImdb_v1.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoCXNaVXgmXN"
   },
   "source": [
    "## Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP5Xpgpcgl0_",
    "outputId": "e9cee4ea-3b1e-4c87-f370-6e7872726c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = list()\n",
    "train_words = list()\n",
    "sorted_words = list()\n",
    "for text in x_train:\n",
    "  tokens = text.split()\n",
    "  train_words.extend(tokens)\n",
    "\n",
    "Counter = Counter(train_words)\n",
    "Counter_copy = Counter\n",
    "temp = Counter.most_common(3998)\n",
    "for key in temp:\n",
    "    sorted_words.append(key[0])\n",
    "\n",
    "#n=99, m = 1000, k = 2898\n",
    "k=list()\n",
    "n=list()\n",
    "m = Counter.most_common(1100)\n",
    "j=0\n",
    "for key in m:\n",
    "  j+=1\n",
    "  if(j>=101):\n",
    "    vocabulary.append(key[0])\n",
    " \n",
    "\n",
    "for i in range(1,100):\n",
    "  n.append(sorted_words[i])\n",
    "j=0\n",
    "for key in sorted_words:\n",
    "  j+=1\n",
    "  if(j<=1100):\n",
    "    sorted_words.remove(key)\n",
    "k = sorted_words.copy()\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDGjmz4UxlRi"
   },
   "source": [
    "## Create binary vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KESrOUVAhmRD",
    "outputId": "077cba85-f8c7-4070-a896-fe7d84e1c4c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:25<00:00, 291.42it/s]\n",
      "100%|██████████| 25000/25000 [01:25<00:00, 291.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x_train_binary = list()\n",
    "x_test_binary = list()\n",
    "\n",
    "for text in tqdm(x_train):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_train_binary.append(binary_vector)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary)\n",
    "\n",
    "for text in tqdm(x_test):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_test_binary.append(binary_vector)\n",
    "\n",
    "x_test_binary = np.array(x_test_binary)\n",
    "#print(x_test_binary[0])\n",
    "y_train_list = y_train.tolist()\n",
    "print(x_train_binary[0])\n",
    "\n",
    "vocabulary_indexes = list()\n",
    "for i in range(len(vocabulary)):\n",
    "  vocabulary_indexes.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh0UFy2FyCW4"
   },
   "source": [
    "## Naive Bayes classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT5E13ejyD2f",
    "outputId": "64341573-8b80-43a2-88b4-7f50c404c8a3"
   },
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "\n",
    "    def __init__(self):\n",
    "        #Λίστες με τις πιθανότητες να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review αντίστοιχα\n",
    "        self.x1_while_c_is_negative = list()\n",
    "        self.x1_while_c_is_positive = list()\n",
    "        #καθολικές μεταβλητές με την πιθανότητα να έχουμε αρνητικό review και θετικό review αντιστοιχα\n",
    "        self.p_c0 = float(0)\n",
    "        self.p_c1 = float(0)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        #Initialisations of elements:\n",
    "        self.x1_while_c_is_negative = []\n",
    "        self.x1_while_c_is_positive = []\n",
    "        reviews = len(Y) \n",
    "        neg_reviews = 0 \n",
    "        pos_reviews = 0\n",
    "        sum_pos =  list() #Σε καθε θέση i του πίνακα: Πόσες φορες εμφανίζεται η λεξη i ενώ έχουμε θετικό review\n",
    "        sum_neg =  list() #Σε καθε θέση i του πίνακα: Πόσες φορες εμφανίζεται η λεξη i ενώ έχουμε αρνητικό review\n",
    "        p_ex_pos = list() #Each element represents for the word Xelement the probability: P( Xelement = 1 | C = 1) \n",
    "        p_ex_neg = list() #Each element represents for the word Xelement the probability: P( Xelement = 1 | C = 0) \n",
    "\n",
    "        #Υπολογισμός της γενικής πιθανότητας να έχουμε θετικό ή αρνητικό review:\n",
    "        for i in range(reviews):\n",
    "            if Y[i] == 0:\n",
    "                neg_reviews += 1\n",
    "            else:\n",
    "                pos_reviews += 1\n",
    "        self.pc0 = pos_reviews/reviews\n",
    "        self.pc1 = neg_reviews/reviews\n",
    "\n",
    "        #Υπολογισμός πιθανοτήτων να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review αντίστοιχα:\n",
    "        \n",
    "\n",
    "        #Αρχικοποίηση λιστών με μετρητές\n",
    "        for i in range(len(vocabulary)):\n",
    "            #βάζουμε ήδη 1 για να αποφύγουμε το να μην υπάρχει κάν μια λέξη\n",
    "            sum_pos.append(0) \n",
    "            sum_neg.append(0)\n",
    "\n",
    "        for i in range(reviews):\n",
    "            for j in range(len(vocabulary)):\n",
    "                if(Y[i] == 0 and X[i][j]==1):\n",
    "                    sum_neg[j] +=1\n",
    "                elif(Y[i] == 1 and X[i][j]==1):\n",
    "                    sum_pos[j] +=1\n",
    "        \n",
    "\n",
    "        #Λίστες πιθανοτήτων να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review \n",
    "        for i in range(0,1000):\n",
    "            p_ex_pos.append(0) \n",
    "            p_ex_neg.append(0)\n",
    "        #for i in range(len(p_ex_neg_train)):\n",
    "        for i in range(len(vocabulary)):\n",
    "            p_ex_neg[i] = (sum_neg[i]+1)/(neg_reviews+2) #P(Xi = 1 | C = 0) \n",
    "        for i in range(len(vocabulary)):\n",
    "            p_ex_pos[i] = (sum_pos[i]+1)/(pos_reviews+2) #P(Xi = 1 | C = 1)\n",
    "\n",
    "        p_ex_neg = np.round(p_ex_neg, decimals=2)\n",
    "        p_ex_pos = np.round(p_ex_pos, decimals=2)\n",
    "\n",
    "        self.x1_while_c_is_positive = p_ex_pos.copy()\n",
    "        self.x1_while_c_is_negative = p_ex_neg.copy()\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # In Naive Bayes classification here we used logarithms to prevent numerical underflow when dealing with probabilities. \n",
    "        # The standard Naive Bayes equation is the following: \n",
    "        # P(Class∣Features) = P(Features∣Class) * P(Class) / P(Features)\n",
    "        # The logarithmic transformation simplifies computations:\n",
    "        # log(P(Class∣Features)) = log(P(Features∣Class)) + log(P(Class)) - log(P(Features))\n",
    "        # This ensures numerical stability and precision in probabilistic models.\n",
    "\n",
    "\n",
    "        predictions = list()\n",
    "        for i in range(len(X)):\n",
    "\n",
    "            # Initialize log probabilities\n",
    "            log_pc0 = np.log(self.pc0)\n",
    "            log_pc1 = np.log(self.pc1)\n",
    "            pc0 = self.pc0\n",
    "            pc1 = self.pc1\n",
    "\n",
    "            # Calculate log probability for negative class (C=0)\n",
    "            for xi in range(len(X[i])):\n",
    "                if X[i][xi] == 0:\n",
    "                    log_pc0 += np.log(1 - self.x1_while_c_is_negative[xi])\n",
    "                    pc0 +=(1 - self.x1_while_c_is_negative[xi])\n",
    "                else:\n",
    "                    log_pc0 += np.log(self.x1_while_c_is_negative[xi])\n",
    "                    pc0 += (self.x1_while_c_is_negative[xi])\n",
    "\n",
    "            # Calculate log probability for positive class (C=1)\n",
    "            for xi in range(len(X[i])):\n",
    "                if X[i][xi] == 1:\n",
    "                    log_pc1 += np.log(self.x1_while_c_is_positive[xi])\n",
    "                    pc1 += (self.x1_while_c_is_positive[xi])\n",
    "                else:\n",
    "                    log_pc1 += np.log(1 - self.x1_while_c_is_positive[xi])\n",
    "                    pc1 += (1 - self.x1_while_c_is_positive[xi])\n",
    "\n",
    "            if log_pc0 < log_pc1:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.948\n",
      "82.588\n",
      "20647\n"
     ]
    }
   ],
   "source": [
    "tool = Naive_Bayes()\n",
    "y_train_list = y_train.tolist()\n",
    "y_test_list = y_test.tolist()\n",
    "tool.fit(x_train_binary, y_train_list)\n",
    "y_pred = tool.predict(x_train_binary)\n",
    "sum=0\n",
    "for i in range(len(y_train_list)):\n",
    "    if(y_train_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_train_list))*100\n",
    "print(correct_percentage_test) \n",
    "\n",
    "# sum = 0\n",
    "# p1 = tool.x1_while_c_is_negative.copy()\n",
    "# p2 = tool.x1_while_c_is_positive.copy()\n",
    "tool2 = Naive_Bayes()\n",
    "# tool2.fit(x_test_binary, y_test_list)\n",
    "y_pred = tool.predict(x_test_binary)\n",
    "sum=0\n",
    "for i in range(len(y_train_list)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_train_list))*100\n",
    "print(correct_percentage_test) \n",
    "\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqYg0fgfyaQY",
    "outputId": "49019cae-25df-4135-a434-c0d9a306b04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82     12500\n",
      "           1       0.81      0.85      0.83     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "nb = Naive_Bayes()\n",
    "nb.fit(x_train_binary, y_train)\n",
    "\n",
    "# Using Naive Bayes Classifier\n",
    "y = nb.predict(x_test_binary)\n",
    "print(classification_report(y_test, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmh2Gxc-ys1h"
   },
   "source": [
    "## ID3 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeOd5C9uyyqI",
    "outputId": "65c0cd08-18e3-4630-b19c-7de464308bab"
   },
   "outputs": [],
   "source": [
    "def IG(Y, Xi):\n",
    "    \n",
    "    pc0=list(Y).count(0)/len(Y) #P(C=0) \n",
    "    pc1=list(Y).count(1)/len(Y) #P(C=1)\n",
    "\n",
    "    #H(C) = -P(C=0)*log2(P(C=0)) -P(C=1)*log2(P(C=1))\n",
    "    Hc= -(pc0) * (math.log(pc0, 2)) -(pc1) * (math.log(pc1, 2))\n",
    "\n",
    "    #P(Xi = 1)*H(C|Xi=1) + P(Xi = 0)*H(C|Xi=0)\n",
    "    sum_calculations = 0 \n",
    "    for digit in range(0,2):\n",
    "\n",
    "        #P(Xi = digit)\n",
    "        p = list(Xi).count(digit)/len(Xi)\n",
    "\n",
    "        #H(C|Xi=digit)\n",
    "        Y_Xi = list()\n",
    "        for i in range(len(Xi)):\n",
    "            if Xi[i]==digit:\n",
    "                Y_Xi.append(Y[i])\n",
    "        if(len(Y_Xi)!=0):\n",
    "            for digit2 in range(0,2):\n",
    "                #H(C=digit2|Xi=digit)\n",
    "                pc = Y_Xi.count(digit2)/len(Y_Xi)\n",
    "                if(pc != 0):\n",
    "                    sum_calculations += p * pc * np.log2(pc)\n",
    "\n",
    "    #IG(Y,Xi)= H(C) - P(Xi = 1)*H(C|Xi=1) + P(Xi = 0)*H(C|Xi=0)\n",
    "    ig = Hc - sum_calculations\n",
    "    return ig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Tree():\n",
    "    def __init__(self):\n",
    "        self.word = \"no word yet\" #Η λέξη με την οποία θα έγινε το classification ενός υπόδεντρου\n",
    "        self.tag = None #1 αν ο κόμβος έχει reviews με τη λέξη με την οποία έγινε το classification, 0 αν δεν την έχουν\n",
    "        self.children = list() #τα παιδία ενός κόμβου\n",
    "        self.classification = int #Η τελική classification. Παίρνει τιμή μόνο αν έχει γίνει \n",
    "    \n",
    "    def new_child(self, node):\n",
    "        self.children.append(node)\n",
    "\n",
    "class ID3():\n",
    "    def __init__(self, max_depth = 10):\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = 0\n",
    "\n",
    "    def most_IG(self, X, Y, vocabulary):\n",
    "\n",
    "        max_gain = -1\n",
    "        max_word= -1\n",
    "\n",
    "\n",
    "        for w in vocabulary:\n",
    "            x_word = list() #Λίστα με όλες τις τιμές που θα πάρει μια λέξη στον Χ\n",
    "            for ex in range(len(X)):\n",
    "                x_word.append(X[ex][w])\n",
    "            word_ig = IG(Y, x_word) #Στέλνουμε το Υ και την λίστα στον ΙG, για να βρει το informtion gain της λέξης ανάλογικά με το Υ\n",
    "\n",
    "            if(word_ig>max_gain):\n",
    "                max_gain = word_ig\n",
    "                max_word = w\n",
    "\n",
    "        return max_word #Η λέξη με το μέγιστο Information Gain\n",
    "\n",
    "    def fit(self, X, Y, vocabulary, default):\n",
    "       \n",
    "        if(len(Y) == 0):\n",
    "            #αν φτάσαμε εδώ τελείωσαν τα Υ γιατί τελείωσε η κατάταξη κάθε review στο δέντρο.\n",
    "            #Παίρνει για τιμή του classification εκείνη που επικρατούσε στο παραπάνω επίπεδο του δέντρο\n",
    "            node = Tree()\n",
    "            node.classification = default \n",
    "            return node \n",
    "\n",
    "        if(len(set(Y)) == 1):\n",
    "            #Η μέθοδος set επιστρέφει ένα set με όλες τις διαφορετικές τιμές που περιλαμβάνει το όρισμα της, εδώ το Υ\n",
    "            #Άρα φτάσαμε εδώ αν το Υ έχει μόνο μια τιμη, η οποία θα χρησιμοποιηθεί στο classification και σταματάει η διαδικασία. \n",
    "            node = Tree()\n",
    "            node.classification = Y[0]\n",
    "            return node\n",
    "\n",
    "        if(len(vocabulary) == 0):\n",
    "            #Αν φτάσαμε εδώ χρησιμοποιήσαμε όλες τις λέξεις οπότε δεν γίνονται παραπάνω κατατάξεις.\n",
    "            #Η διαδικασία σταματάει και γίνεται classified με την τιμή που επικρατεί στα Y\n",
    "            node = Tree()\n",
    "            if(Y.count(0)>Y.count(1)):\n",
    "                max_count = 0\n",
    "            else:\n",
    "                max_count = 1\n",
    "            node.classification = max_count\n",
    "            return node\n",
    "\n",
    "        if (self.depth == self.max_depth):\n",
    "            #Αν είμαστε εδώ φτάσαμε το max depth του δέντρου\n",
    "            #Η διαδικασία σταματάει και γίνεται classified με την τιμή που επικρατεί στα Y. \n",
    "            #Αν έχουμε ισοπαλία αρνητικών θετικών reviews παίρνουμε το default, δηλαδή αυτή που επικρατούσε στο παραπάνω επίπεδο\n",
    "            yes = True\n",
    "            node = Tree()\n",
    "            if((Y.count(0))>Y.count(1)):\n",
    "                node.classification = 0\n",
    "            elif((Y.count(0))<Y.count(1)):\n",
    "                node.classification = 1\n",
    "            else:\n",
    "                node.classification =default\n",
    "            return node\n",
    "\n",
    "        #Σταματάει η διαδικασία αν υπερτερεί στα εναπομείναντα reviews είτε το 0 είτε το 1\n",
    "        if (float(Y.count(1))/float(len(Y))>= 0.95):\n",
    "            node = Tree()\n",
    "            node.classification = 1                   \n",
    "            return node\n",
    "        \n",
    "        if(float(Y.count(0))/float(len(Y))>= 0.95):\n",
    "            node = Tree()\n",
    "            node.classification = 0\n",
    "            return node\n",
    "\n",
    "        #Αποθήκευση του clssification που επικρατεί μέχρι στιγμης ώστε να περασθεί ως default στα παρακάτω επίπεδα\n",
    "        if(Y.count(1)>Y.count(0)):\n",
    "            max_count = 1\n",
    "        else:\n",
    "            max_count = 0\n",
    "\n",
    "\n",
    "        best_word = self.most_IG(X, Y, vocabulary) #Εύρεση της λέξης με το μέγιστο Information Gain \n",
    "        tree = Tree() #Αρχικοποίηση υπόδεντρου\n",
    "\n",
    "        #το νεο λεξιλόγιο, χωρίς την λέξη που θα χρσιμοποιηθέι για τον διαχωρισμό σε φύλλα τωρα ωστέ να μην ξαναχρησιμοποιηθεί μετά\n",
    "        new_vocabulary = vocabulary.copy() \n",
    "        new_vocabulary.remove(best_word)\n",
    "        self.depth += 1 #ενημέρωση του depth\n",
    "\n",
    "        for zero_or_one in range(2):\n",
    "            # if(zero_or_one==0):\n",
    "            #     print(len(new_vocabulary))\n",
    "            #Oι νέες λίστες reviews, δημιουργούνται 2 για κάθε κατηγορία(Υ και Χ) λόγω της for, μια με τη best_word και μία χωρις \n",
    "            x_new = list()\n",
    "            y_new = list()\n",
    "            for i in range(len(X)):\n",
    "                if X[i][best_word] == zero_or_one:\n",
    "                    x_new.append(X[i])\n",
    "                    y_new.append(Y[i])\n",
    "            subtree = self.fit(x_new, y_new, new_vocabulary, max_count)\n",
    "            subtree.tag = zero_or_one \n",
    "            subtree.word = best_word\n",
    "            tree.new_child(subtree)            \n",
    "                \n",
    "        return tree\n",
    "\n",
    "    # def predict_sample(self, x_sample, tree):\n",
    "    #     decided = False\n",
    "    #     sub_tree = tree\n",
    "    #     while not decided:\n",
    "    #         feature = sub_tree.children[0].feature\n",
    "    #         for sub in sub_tree.children:\n",
    "    #             if sub.tag == x_sample[feature]:\n",
    "    #                 sub_tree = sub\n",
    "    #         if (sub_tree.decision == 1 or sub_tree.decision == 0):\n",
    "    #             decided = True\n",
    "    #     return sub_tree.decision\n",
    "\n",
    "    def singular_prediction(self, X, tree):\n",
    "        sub_tree = tree #Αρχικοποίηση Υπόδεντρου\n",
    "        flag = False\n",
    "        while not flag:\n",
    "            word_feature = sub_tree.children[0].word #Παίρνουμε τη λέξη με την οποία έγινε ο διαχωρισμός\n",
    "            for sub in sub_tree.children:\n",
    "                if (sub.tag == X[word_feature]): \n",
    "                    #Αν υπάρχει η λέξη-κριτήριο με την οποία έγινε ο διαχωρισμος σε αυτο το επίπεδο στο sample που κοιταμε \n",
    "                    # πάμε στο υπόδεντρο οπόυ το tag είναι 1(δηλ. έχει reviews που την περιλαμβανουν), αλλιώς σε αυτο που ειναι 0\n",
    "                    sub_tree = sub\n",
    "            if(sub_tree.classification == 1 or sub_tree.classification == 0):\n",
    "                #Σταματάμε αν φτάσουμε σε κάποιο φυλλο. Τα φύλλα έχουν τιμή 0 ή 1 και αυτο καταλήγει να ειναι το classification\n",
    "                # του sample. Οι άλλοι ενδιάμεσοι κόμβοι έχουν None στο classification\n",
    "                flag = True\n",
    "        return sub_tree.classification\n",
    "\n",
    "    def predict(self, tree, X):\n",
    "        y_pred = list()\n",
    "        for i in range(len(X)):\n",
    "            y_pred.append(self.singular_prediction(X[i], tree)) #πρόβλεψη για κάθε review ξεχωριστά\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(x_train_binary, y_train)\n",
    "y_pred = dt.predict(x_test_binary)\n",
    "y_test_list = y_test.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)\n",
    "#print(classification_report(y_train, dt.predict(x_train_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "# clf = ID3()\n",
    "# tree = clf.fit(x_train_binary, y_train, list(range(len(x_train_binary[0]))), 1)\n",
    "# prediction = clf.predict(tree, x_test_binary)\n",
    "#meta th fit:\n",
    "#y_pred = model.predict(trained_tree, x_train_binary)\n",
    "# y_train_list = y_train.tolist()\n",
    "# sum=0\n",
    "# for i in range(len(y_train)):\n",
    "#     if(y_train_list[i]==y_pred[i]):\n",
    "#         sum+=1\n",
    "# correct_percentage_train = (sum/len(y_train_list))*100\n",
    "# print(correct_percentage_train)\n",
    "\n",
    "# print(len(vocabulary_indexes))\n",
    "model = ID3(400)\n",
    "y_train_list = y_train.tolist()\n",
    "y_test_list = y_test.tolist()\n",
    "trained_tree = model.fit(x_train_binary, y_train_list, vocabulary_indexes, 0)\n",
    "\n",
    "y_pred = model.predict(trained_tree, x_train_binary)\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_train_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)\n",
    "\n",
    "#trained_tree = model.fit(x_test_binary, y_test_list, vocabulary_indexes, 0)\n",
    "y_pred = model.predict(trained_tree, x_test_binary)\n",
    "y_test_list = y_test.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNfgLhBNIqLc",
    "outputId": "2e862136-b144-4aba-df7e-dd147a18c973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12500\n",
      "           1       1.00      1.00      1.00     12500\n",
      "\n",
      "    accuracy                           1.00     25000\n",
      "   macro avg       1.00      1.00      1.00     25000\n",
      "weighted avg       1.00      1.00      1.00     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, dt.predict(x_train_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wt6yoSICzPpz",
    "outputId": "a429d3db-2833-43af-9347-2c4f8a0203af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71     12500\n",
      "           1       0.71      0.71      0.71     12500\n",
      "\n",
      "    accuracy                           0.71     25000\n",
      "   macro avg       0.71      0.71      0.71     25000\n",
      "weighted avg       0.71      0.71      0.71     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, dt.predict(x_test_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhNCa4YSzS5J"
   },
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyQtFiv8zUV5",
    "outputId": "d46121a7-eecc-492e-ba8a-7cad33aa614f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Random_Forest():\n",
    "    def __init__(self, num_of_words, trees = 10):\n",
    "        self.num_of_words = num_of_words #Αριθμός των λέξεων\n",
    "        self.trees = trees #Αριθμός των δέντρων που θα φτιαχθούν\n",
    "        self.forest = list() #Λίστα Δέντων\n",
    "\n",
    "    def new_sample(self, X, Y):\n",
    "        #Αρχικοποίηση των νέων x και y\n",
    "        x_new = list()\n",
    "        y_new = list()\n",
    "\n",
    "        y_indexes = list() #Tα indexes των reviews που δεν έχουν επιλεχθεί\n",
    "        for i in range(len(Y)):\n",
    "            y_indexes.append(i)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            #Τυχαία επιλογή reviews για το υποσύνολο που θα επιστρέψει η μέθοδος, χρησιμοποιώντας τα indexes που φτιάχτηκε πάνω\n",
    "            random_choice = random.choice(y_indexes) \n",
    "            x_new.append(X[random_choice])\n",
    "            y_new.append(Y[random_choice])\n",
    "\n",
    "        return x_new, y_new\n",
    "\n",
    "    def new_vocabulary(self, X):\n",
    "        #Λίστα με τα indexes του λεξιλογιου για τυχαία επιλογή των νέων λέξεων του νέου λεξιλογίου που επιστρέφει η μέθοδος \n",
    "        words_indexes = list()\n",
    "        for x in range(len(X[0])):\n",
    "            words_indexes.append(x)\n",
    "\n",
    "        new_words = list()\n",
    "        for i in range(self.num_of_words):\n",
    "            random_word = random.choice(words_indexes) #Tυχαία επιλογή λέξης\n",
    "            words_indexes.remove(random_word) #Αφαίρεση από το παλιό λεξιλόγιο\n",
    "            new_words.append(random_word) #Εισαγωγή στο καινούριο\n",
    "\n",
    "        return new_words\n",
    "\n",
    "    def fit(self, X, Y, max_depth = 10):\n",
    "        for i in range(self.trees):\n",
    "            id3 = ID3(max_depth) #Δημιουργία id3 δέντρου\n",
    "            random_x, random_y = self.new_sample(X, Y)\n",
    "            tree = id3.fit(random_x, random_y, self.new_vocabulary(random_x), 0)\n",
    "            self.forest.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = list()\n",
    "        for i in range(len(X)):\n",
    "            zeros =0\n",
    "            ones = 0\n",
    "            for j in range(self.trees):\n",
    "                id3 = ID3()\n",
    "                prediction = id3.singular_prediction(X[i], self.forest[j])\n",
    "                if (prediction == 1):\n",
    "                    ones += 1\n",
    "                elif(prediction==0):\n",
    "                    zeros +=1\n",
    "            if ones>zeros:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4896\\4269910413.py\", line 2, in <module>\n",
      "    trained_forest = model.fit(x_train_binary, y_train_list)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4896\\3730876292.py\", line 44, in fit\n",
      "    tree = id3.fit(random_x, random_y, self.new_vocabulary(random_x), 0)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4896\\462271505.py\", line 145, in fit\n",
      "    subtree = self.fit(x_new, y_new, new_vocabulary, max_count)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4896\\462271505.py\", line 145, in fit\n",
      "    subtree = self.fit(x_new, y_new, new_vocabulary, max_count)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4896\\462271505.py\", line 145, in fit\n",
      "    subtree = self.fit(x_new, y_new, new_vocabulary, max_count)\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4896\\462271505.py\", line 127, in fit\n",
      "    best_word = self.most_IG(X, Y, vocabulary) #Εύρεση της λέξης με το μέγιστο Information Gain\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4896\\462271505.py\", line 60, in most_IG\n",
      "    word_ig = IG(Y, x_word) #Στέλνουμε το Υ και την λίστα στον ΙG, για να βρει το informtion gain της λέξης ανάλογικά με το Υ\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4896\\462271505.py\", line -1, in IG\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "model = Random_Forest(len(vocabulary))\n",
    "trained_forest = model.fit(x_train_binary, y_train_list)\n",
    "\n",
    "y_pred = model.predict(x_test_binary)\n",
    "y_test_list = y_test.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gT5jtyvszqyM",
    "outputId": "e4819fb5-a389-48a1-ea6e-b257eb92fcbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84     12500\n",
      "           1       0.84      0.84      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, rf.predict(x_test_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgwVr_GAzu9o"
   },
   "source": [
    "## AdaBoost classifier\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffzG0v0Wzv0w",
    "outputId": "4e53ff0b-f198-4f25-eb41-fd35115bdf4e"
   },
   "outputs": [],
   "source": [
    "# Decision stump used as weak classifier\n",
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples =np.shape(X) \n",
    "        print(n_samples)\n",
    "        X_column = X[:, self.feature_idx]\n",
    "        predictions = list()\n",
    "        for i in range(len(n_samples)):\n",
    "            predictions.append(1)\n",
    "        if self.polarity == 1:\n",
    "            for i in range(len(n_samples)):\n",
    "                if(X_column[i] < self.threshold):\n",
    "                    predictions[i] = 0\n",
    "        else:\n",
    "            for i in range(len(n_samples)):\n",
    "                if(X_column[i] > self.threshold):\n",
    "                    predictions[i] = 0\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class Adaboost():\n",
    "\n",
    "    def __init__(self, n_clf=5):\n",
    "        self.n_clf = n_clf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        n_features = len(X)\n",
    "        print(n_features)\n",
    "        # Initialize weights to 1/N\n",
    "        #w = np.full(n_samples, (1 / n_samples)) kanei to idio\n",
    "        w = list()\n",
    "        for i in range(n_samples):\n",
    "            w.append(1/n_samples)\n",
    "\n",
    "        self.clfs = list()\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "\n",
    "            min_error = float(1000000000000)\n",
    "            # greedy search to find best threshold and feature\n",
    "            #for feature_j in \n",
    "            for feature_i in range(n_features):\n",
    "                X_column = X[feature_i]\n",
    "                print(len(X_column)) \n",
    "                #print(feature_i)\n",
    "                thresholds = np.unique(X_column)\n",
    "                #print(X_column)\n",
    "                #print(X)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    # predict with polarity 1\n",
    "                    p = 1\n",
    "                    predictions = list()\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[X_column < threshold] = -1\n",
    "                    #for i in range(n_samples):\n",
    "                    #    predictions.append(1)\n",
    "                    #for i in range(n_samples):\n",
    "                    #    if(X_column < threshold):\n",
    "                    #        predictions[i] = 0\n",
    "\n",
    "                    print(predictions)\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    misclassified = list()\n",
    "                    for i in range(n_samples):\n",
    "                        if (predictions[i]!=y):\n",
    "                            misclassified.append(w[i])\n",
    "                    error = 0\n",
    "                    for i in range(len(misclassified)):\n",
    "                        error += misclassified[i]\n",
    "\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # store the best configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_idx = feature_i\n",
    "                        min_error = error\n",
    "\n",
    "            # calculate alpha\n",
    "            EPS = 1e-10\n",
    "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "            # calculate predictions and update weights\n",
    "            predictions = clf.predict(X)\n",
    "\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds, axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    sum=0\n",
    "    for i in range(len(y_true)):\n",
    "        if(y_true[i]==y_pred[i]):\n",
    "            sum+=1\n",
    "    correct_percentage_test = (sum/y_true)*100\n",
    "    return correct_percentage_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "1000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 25000 but corresponding boolean dimension is 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ada \u001b[38;5;241m=\u001b[39m Adaboost()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mada\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m ada\u001b[38;5;241m.\u001b[39mpredict(x_test_binary)\n\u001b[0;32m      5\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy(y_train, y_pred)\n",
      "Cell \u001b[1;32mIn[50], line 64\u001b[0m, in \u001b[0;36mAdaboost.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     62\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     63\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(n_samples)\n\u001b[1;32m---> 64\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_column\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#for i in range(n_samples):\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#    predictions.append(1)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#for i in range(n_samples):\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#    if(X_column < threshold):\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#        predictions[i] = 0\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 25000 but corresponding boolean dimension is 1000"
     ]
    }
   ],
   "source": [
    "ada = Adaboost()\n",
    "ada.fit(x_train_binary, y_train)\n",
    "y_pred = ada.predict(x_test_binary)\n",
    "\n",
    "acc = accuracy(y_train, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toInvux_0Ue6",
    "outputId": "ca86519e-787b-4671-bf5f-29a5716d8125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80     12500\n",
      "           1       0.79      0.84      0.81     12500\n",
      "\n",
      "    accuracy                           0.81     25000\n",
      "   macro avg       0.81      0.81      0.81     25000\n",
      "weighted avg       0.81      0.81      0.81     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, ab.predict(x_test_binary)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sklearn-IMDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

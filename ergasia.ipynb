{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeJxAKSvNfNy"
   },
   "source": [
    "# IMDB sentiment analysis with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhIK8UDAgpMu"
   },
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8RMZkur0Z7H4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math \n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=4000)\n",
    "\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quKpVbcFEhZT",
    "outputId": "30438610-f123-4e4a-e09c-a21115540867"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "AOEmWY7yfBFu",
    "outputId": "7a134933-2827-4e02-dd3f-651ce21c77d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[bos] this film was just brilliant casting location scenery story direction [oov] really suited the part they played and you could just imagine being there robert [oov] is an amazing actor and now the same being director [oov] father came from the same [oov] island as myself so i loved the fact there was a real connection with this film the witty [oov] throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for [oov] and would recommend it to everyone to watch and the fly [oov] was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also [oov] to the two little [oov] that played the [oov] of norman and paul they were just brilliant children are often left out of the [oov] list i think because the stars that play them all grown up are such a big [oov] for the whole film but these children are amazing and should be [oov] for what they have done don't you think the whole story was so lovely because it was true and was [oov] life after all that was [oov] with us all\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYTO-LIY0j-h"
   },
   "source": [
    "## Alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DRkm4epU0moB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open ''aclImdb_v1.tar.gz''\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xvf  'aclImdb_v1.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoCXNaVXgmXN"
   },
   "source": [
    "## Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP5Xpgpcgl0_",
    "outputId": "e9cee4ea-3b1e-4c87-f370-6e7872726c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = list()\n",
    "train_words = list()\n",
    "sorted_words = list()\n",
    "for text in x_train:\n",
    "  tokens = text.split()\n",
    "  train_words.extend(tokens)\n",
    "\n",
    "Counter = Counter(train_words)\n",
    "Counter_copy = Counter\n",
    "temp = Counter.most_common(3998)\n",
    "for key in temp:\n",
    "    sorted_words.append(key[0])\n",
    "\n",
    "#n=99, m = 1000, k = 2898\n",
    "k=list()\n",
    "n=list()\n",
    "m = Counter.most_common(1100)\n",
    "j=0\n",
    "for key in m:\n",
    "  j+=1\n",
    "  if(j>=101):\n",
    "    vocabulary.append(key[0])\n",
    " \n",
    "\n",
    "for i in range(1,100):\n",
    "  n.append(sorted_words[i])\n",
    "j=0\n",
    "for key in sorted_words:\n",
    "  j+=1\n",
    "  if(j<=1100):\n",
    "    sorted_words.remove(key)\n",
    "k = sorted_words.copy()\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDGjmz4UxlRi"
   },
   "source": [
    "## Create binary vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KESrOUVAhmRD",
    "outputId": "077cba85-f8c7-4070-a896-fe7d84e1c4c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:15<00:00, 330.10it/s]\n",
      "100%|██████████| 25000/25000 [01:14<00:00, 337.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x_train_binary = list()\n",
    "x_test_binary = list()\n",
    "\n",
    "for text in tqdm(x_train):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_train_binary.append(binary_vector)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary)\n",
    "\n",
    "for text in tqdm(x_test):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_test_binary.append(binary_vector)\n",
    "\n",
    "x_test_binary = np.array(x_test_binary)\n",
    "#print(x_test_binary[0])\n",
    "y_train_list = y_train.tolist()\n",
    "print(x_train_binary[0])\n",
    "\n",
    "vocabulary_indexes = list()\n",
    "for i in range(len(vocabulary)):\n",
    "  vocabulary_indexes.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh0UFy2FyCW4"
   },
   "source": [
    "## Naive Bayes classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT5E13ejyD2f",
    "outputId": "64341573-8b80-43a2-88b4-7f50c404c8a3"
   },
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "    def fit(self,X,y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "        \n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        for c in self._classes:\n",
    "            X_c = X[c==y]\n",
    "            self._mean[c,:] = X_c.mean(axis=0)\n",
    "            self._var[c,:] = X_c.var(axis=0)\n",
    "            self._priors[c] = X_c.shape[0] / float(n_samples)\n",
    "                        \n",
    "    def predict(self,X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return y_pred\n",
    "    \n",
    "    def _predict(self,x):\n",
    "        post = []   \n",
    "        for idx ,c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            cond = np.sum(np.log(self._pred(idx,x)))\n",
    "            prt = prior + cond\n",
    "            post.append(prt)\n",
    "        return self._classes[np.argmax(post)]\n",
    "    \n",
    "    def _pred(self,class_idx,x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        num = np.exp(-(x-mean)**2 / (2*var))\n",
    "        den = np.sqrt(2* np.pi * var)\n",
    "        return num / den\n",
    "\n",
    "\n",
    "\n",
    "# class Naive_Bayes:\n",
    "\n",
    "#     def __init__(self):\n",
    "#         #Λίστες με τις πιθανότητες να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review αντίστοιχα\n",
    "#         self.x1_while_c_is_negative = list()\n",
    "#         self.x1_while_c_is_positive = list()\n",
    "#         #καθολικές μεταβλητές με την πιθανότητα να έχουμε αρνητικό review και θετικό review αντιστοιχα\n",
    "#         self.p_c0 = float(0)\n",
    "#         self.p_c1 = float(0)\n",
    "\n",
    "#     def fit(self, X, Y):\n",
    "#         #Initialisations of elements:\n",
    "#         self.x1_while_c_is_negative = []\n",
    "#         self.x1_while_c_is_positive = []\n",
    "#         reviews = len(Y) \n",
    "#         neg_reviews = 0 \n",
    "#         pos_reviews = 0\n",
    "#         sum_pos =  list() #Σε καθε θέση i του πίνακα: Πόσες φορες εμφανίζεται η λεξη i ενώ έχουμε θετικό review\n",
    "#         sum_neg =  list() #Σε καθε θέση i του πίνακα: Πόσες φορες εμφανίζεται η λεξη i ενώ έχουμε αρνητικό review\n",
    "#         p_ex_pos = list() #Each element represents for the word Xelement the probability: P( Xelement = 1 | C = 1) \n",
    "#         p_ex_neg = list() #Each element represents for the word Xelement the probability: P( Xelement = 1 | C = 0) \n",
    "\n",
    "#         #Υπολογισμός της γενικής πιθανότητας να έχουμε θετικό ή αρνητικό review:\n",
    "#         for i in range(reviews):\n",
    "#             if Y[i] == 0:\n",
    "#                 neg_reviews += 1\n",
    "#             else:\n",
    "#                 pos_reviews += 1\n",
    "#         self.pc0 = pos_reviews/reviews\n",
    "#         self.pc1 = neg_reviews/reviews\n",
    "\n",
    "#         #Υπολογισμός πιθανοτήτων να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review αντίστοιχα:\n",
    "        \n",
    "\n",
    "#         #Αρχικοποίηση λιστών με μετρητές\n",
    "#         for i in range(len(vocabulary)):\n",
    "#             #βάζουμε ήδη 1 για να αποφύγουμε το να μην υπάρχει κάν μια λέξη\n",
    "#             sum_pos.append(0) \n",
    "#             sum_neg.append(0)\n",
    "\n",
    "#         for i in range(reviews):\n",
    "#             for j in range(len(vocabulary)):\n",
    "#                 if(Y[i] == 0 and X[i][j]==1):\n",
    "#                     sum_neg[j] +=1\n",
    "#                 elif(Y[i] == 1 and X[i][j]==1):\n",
    "#                     sum_pos[j] +=1\n",
    "        \n",
    "\n",
    "#         #Λίστες πιθανοτήτων να έχουμε την κάθε λέξη δεδομένου πως έχουμε αρνητικό ή θετικό review \n",
    "#         for i in range(0,1000):\n",
    "#             p_ex_pos.append(0) \n",
    "#             p_ex_neg.append(0)\n",
    "#         #for i in range(len(p_ex_neg_train)):\n",
    "#         for i in range(len(vocabulary)):\n",
    "#             p_ex_neg[i] = (sum_neg[i]+1)/(neg_reviews+2) #P(Xi = 1 | C = 0) \n",
    "#         for i in range(len(vocabulary)):\n",
    "#             p_ex_pos[i] = (sum_pos[i]+1)/(pos_reviews+2) #P(Xi = 1 | C = 1)\n",
    "\n",
    "#         p_ex_neg = np.round(p_ex_neg, decimals=2)\n",
    "#         p_ex_pos = np.round(p_ex_pos, decimals=2)\n",
    "\n",
    "#         self.x1_while_c_is_positive = p_ex_pos.copy()\n",
    "#         self.x1_while_c_is_negative = p_ex_neg.copy()\n",
    "\n",
    "#     def predict(self, X):\n",
    "\n",
    "#         # In Naive Bayes classification here we used logarithms to prevent numerical underflow when dealing with probabilities. \n",
    "#         # The standard Naive Bayes equation is the following: \n",
    "#         # P(Class∣Features) = P(Features∣Class) * P(Class) / P(Features)\n",
    "#         # The logarithmic transformation simplifies computations:\n",
    "#         # log(P(Class∣Features)) = log(P(Features∣Class)) + log(P(Class)) - log(P(Features))\n",
    "#         # This ensures numerical stability and precision in probabilistic models.\n",
    "\n",
    "\n",
    "#         predictions = list()\n",
    "#         for i in range(len(X)):\n",
    "\n",
    "#             # Initialize log probabilities\n",
    "#             log_pc0 = np.log(self.pc0)\n",
    "#             log_pc1 = np.log(self.pc1)\n",
    "#             pc0 = self.pc0\n",
    "#             pc1 = self.pc1\n",
    "\n",
    "#             # Calculate log probability for negative class (C=0)\n",
    "#             for xi in range(len(X[i])):\n",
    "#                 if X[i][xi] == 0:\n",
    "#                     log_pc0 += np.log(1 - self.x1_while_c_is_negative[xi])\n",
    "#                     pc0 +=(1 - self.x1_while_c_is_negative[xi])\n",
    "#                 else:\n",
    "#                     log_pc0 += np.log(self.x1_while_c_is_negative[xi])\n",
    "#                     pc0 += (self.x1_while_c_is_negative[xi])\n",
    "\n",
    "#             # Calculate log probability for positive class (C=1)\n",
    "#             for xi in range(len(X[i])):\n",
    "#                 if X[i][xi] == 1:\n",
    "#                     log_pc1 += np.log(self.x1_while_c_is_positive[xi])\n",
    "#                     pc1 += (self.x1_while_c_is_positive[xi])\n",
    "#                 else:\n",
    "#                     log_pc1 += np.log(1 - self.x1_while_c_is_positive[xi])\n",
    "#                     pc1 += (1 - self.x1_while_c_is_positive[xi])\n",
    "\n",
    "#             if log_pc0 < log_pc1:\n",
    "#                 predictions.append(1)\n",
    "#             else:\n",
    "#                 predictions.append(0)\n",
    "\n",
    "#         return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.844\n",
      "81.896\n",
      "20474\n"
     ]
    }
   ],
   "source": [
    "tool = Naive_Bayes()\n",
    "y_train_list = y_train.tolist()\n",
    "y_test_list = y_test.tolist()\n",
    "tool.fit(x_train_binary, y_train_list)\n",
    "y_pred = tool.predict(x_train_binary)\n",
    "sum=0\n",
    "for i in range(len(y_train_list)):\n",
    "    if(y_train_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_train_list))*100\n",
    "print(correct_percentage_test) \n",
    "\n",
    "# sum = 0\n",
    "# p1 = tool.x1_while_c_is_negative.copy()\n",
    "# p2 = tool.x1_while_c_is_positive.copy()\n",
    "tool2 = Naive_Bayes()\n",
    "# tool2.fit(x_test_binary, y_test_list)\n",
    "y_pred = tool.predict(x_test_binary)\n",
    "sum=0\n",
    "for i in range(len(y_train_list)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_train_list))*100\n",
    "print(correct_percentage_test) \n",
    "\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqYg0fgfyaQY",
    "outputId": "49019cae-25df-4135-a434-c0d9a306b04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82     12500\n",
      "           1       0.82      0.82      0.82     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "nb = Naive_Bayes()\n",
    "nb.fit(x_train_binary, y_train)\n",
    "\n",
    "# Using Naive Bayes Classifier\n",
    "y = nb.predict(x_test_binary)\n",
    "print(classification_report(y_test, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmh2Gxc-ys1h"
   },
   "source": [
    "## ID3 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeOd5C9uyyqI",
    "outputId": "65c0cd08-18e3-4630-b19c-7de464308bab"
   },
   "outputs": [],
   "source": [
    "#The formula for calculating the entropy is:\n",
    "#H(C) = -P(C=0)*log2(P(C=0)) -P(C=1)*log2(P(C=1))\n",
    "#And it is used in the calculation of the Information Gain as follows:\n",
    "#IG(Y,Xi)= H(C) - P(Xi = 1)*H(C|Xi=1) + P(Xi = 0)*H(C|Xi=0)\n",
    "#Which, if we want to alanlyze more, becomes:\n",
    "#IG(Y,Xi)= H(C) - \n",
    "#         (P(X=1)*( -(P(C=1|X=1)*log2(P(C=1|X=1))) - (P(C=0|X=1)*log2(P(C=0|X=1))) +\n",
    "#\t\t   P(X=0)*( -(P(C=1|X=0)*log2(P(C=1|X=0))) - (P(C=0|X=0)*log2(P(C=0|X=0))))\n",
    "\n",
    "def IG(Y, Xi):\n",
    "\n",
    "    #Part A: Calculating the H(C) = -P(C=0)*log2(P(C=0)) -P(C=1)*log2(P(C=1))\n",
    "    Hc = 0\n",
    "    for c in range(2):\n",
    "        pc = list(Y).count(c)/len(Y) #P(C=Ci) = (Number of Ci)/(All C instances)\n",
    "        Hc += - pc * math.log(pc, 2)\n",
    "\n",
    "    #Part B: Calculating the IG(Y,Xi)= H(C) - P(Xi = 1)*H(C|Xi=1) + P(Xi = 0)*H(C|Xi=0)\n",
    "    #this will happen in two repetitions: one for Xi=0 and one for Xi=1\n",
    "    Hc_second = 0\n",
    "    for feature in range(2):\n",
    "        p = list(Xi).count(feature)/len(Xi) #P(X = i) = (Number of Xi=feat)/(All features of Xi)\n",
    "        \n",
    "        #finding the number of Y instances for which X=Xi, which will be used for calculating the P(C=c|X=x)\n",
    "        Ys = list()  \n",
    "        for i in range(len(Xi)):\n",
    "            if Xi[i] == feature:\n",
    "                Ys.append(Y[i])\n",
    "\n",
    "        #Calculating the H(C=c|X=x) = ( -(P(C=1|X=x)*log2(P(C=1|X=x))) - (P(C=0|X=x)*log2(P(C=0|X=x))) (for x in [0,1])\n",
    "        for c in range(2):\n",
    "            if len(Ys)!=0:\n",
    "                pc_while_x = Ys.count(c)/len(Ys) #P(C=c|X=x) = (Instances of C for which C=c and X=x)/(All instances of C for which X=x)\n",
    "                if pc_while_x != 0:\n",
    "                    H = - p * pc_while_x * math.log(pc_while_x, 2)\n",
    "                    Hc_second += H\n",
    "    ig = Hc - Hc_second\n",
    "    return ig    \n",
    "\n",
    "\n",
    "class Tree():\n",
    "    def __init__(self):\n",
    "        self.word = \"no word yet\" #Η λέξη με την οποία θα έγινε το classification ενός υπόδεντρου\n",
    "        self.tag = None #1 αν ο κόμβος έχει reviews με τη λέξη με την οποία έγινε το classification, 0 αν δεν την έχουν\n",
    "        self.children = list() #τα παιδία ενός κόμβου\n",
    "        self.classification = int #Η τελική classification. Παίρνει τιμή μόνο αν έχει γίνει \n",
    "    \n",
    "    def new_child(self, node):\n",
    "        self.children.append(node)\n",
    "\n",
    "class ID3():\n",
    "    def __init__(self, max_depth = 10):\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = 0\n",
    "\n",
    "    def most_IG(self, X, Y, vocabulary):\n",
    "\n",
    "        max_gain = -1\n",
    "        max_word= -1\n",
    "\n",
    "\n",
    "        for w in vocabulary:\n",
    "            x_word = list() #Λίστα με όλες τις τιμές που θα πάρει μια λέξη στον Χ\n",
    "            for ex in range(len(X)):\n",
    "                x_word.append(X[ex][w])\n",
    "            word_ig = IG(Y, x_word) #Στέλνουμε το Υ και την λίστα στον ΙG, για να βρει το informtion gain της λέξης ανάλογικά με το Υ\n",
    "\n",
    "            if(word_ig>max_gain):\n",
    "                max_gain = word_ig\n",
    "                max_word = w\n",
    "\n",
    "        return max_word #Η λέξη με το μέγιστο Information Gain\n",
    "\n",
    "    def fit(self, X, Y, vocabulary, default):\n",
    "       \n",
    "        if(len(Y) == 0):\n",
    "            #αν φτάσαμε εδώ τελείωσαν τα Υ γιατί τελείωσε η κατάταξη κάθε review στο δέντρο.\n",
    "            #Παίρνει για τιμή του classification εκείνη που επικρατούσε στο παραπάνω επίπεδο του δέντρο\n",
    "            node = Tree()\n",
    "            node.classification = default \n",
    "            return node \n",
    "\n",
    "        if(len(set(Y)) == 1):\n",
    "            #Η μέθοδος set επιστρέφει ένα set με όλες τις διαφορετικές τιμές που περιλαμβάνει το όρισμα της, εδώ το Υ\n",
    "            #Άρα φτάσαμε εδώ αν το Υ έχει μόνο μια τιμη, η οποία θα χρησιμοποιηθεί στο classification και σταματάει η διαδικασία. \n",
    "            node = Tree()\n",
    "            node.classification = Y[0]\n",
    "            return node\n",
    "\n",
    "        if(len(vocabulary) == 0):\n",
    "            #Αν φτάσαμε εδώ χρησιμοποιήσαμε όλες τις λέξεις οπότε δεν γίνονται παραπάνω κατατάξεις.\n",
    "            #Η διαδικασία σταματάει και γίνεται classified με την τιμή που επικρατεί στα Y\n",
    "            node = Tree()\n",
    "            if(Y.count(0)>Y.count(1)):\n",
    "                max_count = 0\n",
    "            else:\n",
    "                max_count = 1\n",
    "            node.classification = max_count\n",
    "            return node\n",
    "\n",
    "        if (self.depth == self.max_depth):\n",
    "            #Αν είμαστε εδώ φτάσαμε το max depth του δέντρου\n",
    "            #Η διαδικασία σταματάει και γίνεται classified με την τιμή που επικρατεί στα Y. \n",
    "            #Αν έχουμε ισοπαλία αρνητικών θετικών reviews παίρνουμε το default, δηλαδή αυτή που επικρατούσε στο παραπάνω επίπεδο\n",
    "            yes = True\n",
    "            node = Tree()\n",
    "            if((Y.count(0))>Y.count(1)):\n",
    "                node.classification = 0\n",
    "            elif((Y.count(0))<Y.count(1)):\n",
    "                node.classification = 1\n",
    "            else:\n",
    "                node.classification =default\n",
    "            return node\n",
    "\n",
    "        #Σταματάει η διαδικασία αν υπερτερεί στα εναπομείναντα reviews είτε το 0 είτε το 1\n",
    "        if (float(Y.count(1))/float(len(Y))>= 0.75):\n",
    "            node = Tree()\n",
    "            node.classification = 1                   \n",
    "            return node\n",
    "        \n",
    "        if(float(Y.count(0))/float(len(Y))>= 0.75):\n",
    "            node = Tree()\n",
    "            node.classification = 0\n",
    "            return node\n",
    "\n",
    "        #Αποθήκευση του clssification που επικρατεί μέχρι στιγμης ώστε να περασθεί ως default στα παρακάτω επίπεδα\n",
    "        if(Y.count(1)>Y.count(0)):\n",
    "            max_count = 1\n",
    "        else:\n",
    "            max_count = 0\n",
    "\n",
    "\n",
    "        best_word = self.most_IG(X, Y, vocabulary) #Εύρεση της λέξης με το μέγιστο Information Gain \n",
    "        tree = Tree() #Αρχικοποίηση υπόδεντρου\n",
    "\n",
    "        #το νεο λεξιλόγιο, χωρίς την λέξη που θα χρσιμοποιηθέι για τον διαχωρισμό σε φύλλα τωρα ωστέ να μην ξαναχρησιμοποιηθεί μετά\n",
    "        new_vocabulary = vocabulary.copy() \n",
    "        new_vocabulary.remove(best_word)\n",
    "        self.depth += 1 #ενημέρωση του depth\n",
    "\n",
    "        for zero_or_one in range(2):\n",
    "            # if(zero_or_one==0):\n",
    "            #     print(len(new_vocabulary))\n",
    "            #Oι νέες λίστες reviews, δημιουργούνται 2 για κάθε κατηγορία(Υ και Χ) λόγω της for, μια με τη best_word και μία χωρις \n",
    "            x_new = list()\n",
    "            y_new = list()\n",
    "            for i in range(len(X)):\n",
    "                if X[i][best_word] == zero_or_one:\n",
    "                    x_new.append(X[i])\n",
    "                    y_new.append(Y[i])\n",
    "            subtree = self.fit(x_new, y_new, new_vocabulary, max_count)\n",
    "            subtree.tag = zero_or_one \n",
    "            subtree.word = best_word\n",
    "            tree.new_child(subtree)            \n",
    "                \n",
    "        return tree\n",
    "\n",
    "    def singular_prediction(self, X, tree):\n",
    "        sub_tree = tree #Αρχικοποίηση Υπόδεντρου\n",
    "        flag = False\n",
    "        while not flag:\n",
    "            word_feature = sub_tree.children[0].word #Παίρνουμε τη λέξη με την οποία έγινε ο διαχωρισμός\n",
    "            for sub in sub_tree.children:\n",
    "                if (sub.tag == X[word_feature]): \n",
    "                    #Αν υπάρχει η λέξη-κριτήριο με την οποία έγινε ο διαχωρισμος σε αυτο το επίπεδο στο sample που κοιταμε \n",
    "                    # πάμε στο υπόδεντρο οπόυ το tag είναι 1(δηλ. έχει reviews που την περιλαμβανουν), αλλιώς σε αυτο που ειναι 0\n",
    "                    sub_tree = sub\n",
    "            if(sub_tree.classification == 1 or sub_tree.classification == 0):\n",
    "                #Σταματάμε αν φτάσουμε σε κάποιο φυλλο. Τα φύλλα έχουν τιμή 0 ή 1 και αυτο καταλήγει να ειναι το classification\n",
    "                # του sample. Οι άλλοι ενδιάμεσοι κόμβοι έχουν None στο classification\n",
    "                flag = True\n",
    "        return sub_tree.classification\n",
    "\n",
    "    def predict(self, tree, X):\n",
    "        y_pred = list()\n",
    "        for i in range(len(X)):\n",
    "            y_pred.append(self.singular_prediction(X[i], tree)) #πρόβλεψη για κάθε review ξεχωριστά\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.708\n",
      "74.79599999999999\n"
     ]
    }
   ],
   "source": [
    "model = ID3(400)\n",
    "y_train_list = y_train.tolist()\n",
    "y_test_list = y_test.tolist()\n",
    "trained_tree = model.fit(x_train_binary, y_train_list, vocabulary_indexes, 0)\n",
    "\n",
    "y_pred = model.predict(trained_tree, x_train_binary)\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_train_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)\n",
    "\n",
    "#trained_tree = model.fit(x_test_binary, y_test_list, vocabulary_indexes, 0)\n",
    "y_pred = model.predict(trained_tree, x_test_binary)\n",
    "y_test_list = y_test.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhNCa4YSzS5J"
   },
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyQtFiv8zUV5",
    "outputId": "d46121a7-eecc-492e-ba8a-7cad33aa614f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Random_Forest():\n",
    "    def __init__(self, num_of_words, trees = 10):\n",
    "        self.num_of_words = num_of_words #Αριθμός των λέξεων\n",
    "        self.trees = trees #Αριθμός των δέντρων που θα φτιαχθούν\n",
    "        self.forest = list() #Λίστα Δέντων\n",
    "\n",
    "    def new_sample(self, X, Y):\n",
    "        #Αρχικοποίηση των νέων x και y\n",
    "        x_new = list()\n",
    "        y_new = list()\n",
    "\n",
    "        y_indexes = list() #Tα indexes των reviews που δεν έχουν επιλεχθεί\n",
    "        for i in range(len(Y)):\n",
    "            y_indexes.append(i)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            #Τυχαία επιλογή reviews για το υποσύνολο που θα επιστρέψει η μέθοδος, χρησιμοποιώντας τα indexes που φτιάχτηκε πάνω\n",
    "            random_choice = random.choice(y_indexes) \n",
    "            x_new.append(X[random_choice])\n",
    "            y_new.append(Y[random_choice])\n",
    "\n",
    "        return x_new, y_new\n",
    "\n",
    "    def new_vocabulary(self, X):\n",
    "        #Λίστα με τα indexes του λεξιλογιου για τυχαία επιλογή των νέων λέξεων του νέου λεξιλογίου που επιστρέφει η μέθοδος \n",
    "        words_indexes = list()\n",
    "        for x in range(len(X[0])):\n",
    "            words_indexes.append(x)\n",
    "\n",
    "        new_words = list()\n",
    "        for i in range(self.num_of_words):\n",
    "            random_word = random.choice(words_indexes) #Tυχαία επιλογή λέξης\n",
    "            words_indexes.remove(random_word) #Αφαίρεση από το παλιό λεξιλόγιο\n",
    "            new_words.append(random_word) #Εισαγωγή στο καινούριο\n",
    "\n",
    "        return new_words\n",
    "\n",
    "    def fit(self, X, Y, max_depth = 10):\n",
    "        for i in range(self.trees):\n",
    "            id3 = ID3(max_depth) #Δημιουργία id3 δέντρου\n",
    "            random_x, random_y = self.new_sample(X, Y)\n",
    "            tree = id3.fit(random_x, random_y, self.new_vocabulary(random_x), 0)\n",
    "            self.forest.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = list()\n",
    "        for i in range(len(X)):\n",
    "            zeros =0\n",
    "            ones = 0\n",
    "            for j in range(self.trees):\n",
    "                id3 = ID3()\n",
    "                prediction = id3.singular_prediction(X[i], self.forest[j])\n",
    "                if (prediction == 1):\n",
    "                    ones += 1\n",
    "                elif(prediction==0):\n",
    "                    zeros +=1\n",
    "            if ones>zeros:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.244\n",
      "71.408\n"
     ]
    }
   ],
   "source": [
    "model = Random_Forest(len(vocabulary))\n",
    "trained_forest = model.fit(x_train_binary, y_train_list)\n",
    "y_pred = model.predict(x_train_binary)\n",
    "y_train_list = y_train.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_train_list))*100\n",
    "print(correct_percentage_test)\n",
    "\n",
    "y_pred = model.predict(x_test_binary)\n",
    "y_test_list = y_test.tolist()\n",
    "sum=0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_list[i]==y_pred[i]):\n",
    "        sum+=1\n",
    "correct_percentage_test = (sum/len(y_test_list))*100\n",
    "print(correct_percentage_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sklearn-IMDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b1f2b33e866b0bf2409397e5f58ba9cdf170d3b7f64c8f359c79998e2f88ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
